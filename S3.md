# Simple Storage Service - S3 - https://aws.amazon.com/s3/faqs/
- durable, highly scalable object storage
- **cannot be used to store OS, db etc - that is block-based storage**
- webservice interface to store and retrieve
- file size - 0 to 5 TB
- files are stored in buckets
- **it has a universal namespace - must be unique globally**
- **when S3 is selected, region becomes global**
- sample url of a bucket - https://s3-eu-west-1.amazonaws.com/smalali
- when you upload a file, you get a http 200
- by default buckets are private and all objects inside the bucket are private
- encryption
  - client side
  - server side
    - server side encryption with Amazon S3 managed keys (SSE-S3)
    - server side encryption with KMS (SSE-KMS)
    - server side encryption with Customer provided keys (SSE-C)

- s3 is a key value store
- it is object based - object consists of -
  - key - name of object
  - value - the data of the object - sequence of bytes
  - version id
  - metadata - ownership data etc
  - subresources:
    - access control lists
    - torrent

- guaranteed 99.99% availability - 99.999999999% durability for s3 information
- tiered storage available, lifecycle management -
- versioning, encryption
- secure data using access control lists and bucket policies
- supports MFA/2FA while deleting objects
- **storage tiers can be associated at either individual object level or bucket level**
- s3 access logs can be enabled to track all access

**Data consistency model for S3**
- read after write consistency for PUTS of new objects - can read the file immediately after writing to s3
- eventual consistency for overwrite PUTS and DELETES (can take time to propagate)

**Storage tiers/classes**
- S3 standard
  - 99.99% availability
  - 99.999999999% durability, stored across multiple devices (discs) in multiple AZs
  - can sustain loss of 2 facilities concurrently (AZs)
- S3 - IA (infrequently accessed)
  - for data that is less frequently accessed, but required rapid access when needed
  - lower fee than S3 standard, but retrieval is charged
  - stored across multiple devices/AZs like standard
  - **also called Reduced redundancy storage**
- S3 Intelligent Tiering
  - applies ML algos to automatically move objects around different classes based on usage
  - helps in optimizing storage costs
- S3 One Zone - IA
  - lower cost option for infrequently accessed data
  - stored in only one AZ
  - used to store non critical data that can be recreated if lost
- S3 Glacier - very cheap, used for archival only
  - Expedited - restored within few mins - costlier
  - Standard - retrieval takes 3 to 5 hours
  - Bulk  - retrieval takes 5 to 12 hours
- S3 Glacier Deep Archive
  - lowest cost storage
  - retrieval time of 12 hrs
![alt text](resources/S3_Storage_Classes.png "S3 Storage Classes")

**S3 charges**
- **charged for storage**
- **no of requests**
- storage management pricing (metadata)
- data transfer pricing
- **transfer acceleration**
- **cross region replication**

**S3 versioning**
- stores all version of object (including all writes and also deletes)
- **once versioning is enabled, it cannot be disabled, only suspended**
- to disable versioning, the bucket has to be deleted and re-created
- **new versions when created are private by default - so you have to make each version public individually**
- you have to pay for each version stored
- can use MFA for deleting a file
- integrates with lifecycle rules
- size of a bucket is the sum of all versions of all the files in the bucket
- to delete a file, you have to delete all the versions
- when you delete a file, it actually creates a new version and calls it Delete Marker
- to restore a file, just delete the Delete Marker version
- when you make the latest version private, the url of the file DOES NOT automatically show the latest public version
- the previous versions can be accessed directly using the version id in the url (if they are public) - https://s3.amazonaws.com/smalali2-version/hello.txt?versionId=ZxTwfTV_F9ltFkpFX0n0EPzR7c9y5Zoz

**Lifecycle Management**
- Used to create/manage policies that are used to transition objects from S3 to S3-IA to Glacier (based on requirements)
- can also have policies to expire objects
- lifecycle mgmt can be applied to both current and previous version of objects (if versioning is enabled)
- following kinds of rules can be set - can be applied on both current and previous versions
  - transition from standard to standard-infrequent access storage after 30 days of creation
  - transition from standard-IA to Glacier storage after 60 days of object creation
  - permanently delete the object after 425 days of object creation

**Object Lock**
- objects can be stored using Write Once Read Many (WORM) model
- prevents objects from being deleted - either for fixed time or indefinitely
- mainly used for regulatory purpose
- can be applied both to individual object or whole bucket
- comes in 2 modes -
  - Governance mode - users cannot delete or overwrite object version or alter lock setting unless they have special permission
  - Compliance mode - object cannot be deleted or overwritten by any user including root user
- S3 Glacier Vault lock
  - applies to Glacier
  - can specify WORM in a Vault lock policy
  - once policy is set, it cannot be changed

**Security**
- by default, all buckets are private
- you can set up access control using - bucket policies, ACLs
- ACLs are granular till the object level
- you can set up access logs to the buckets to see who made requests to the buckets
- can restrict access to users and groups by setting up IAM policies

**Encryption**
- in transit - uses SSL/TLS
- at rest
  - server side encryption
    - S3 Managed Keys - SSE-S3
      - data is encrypted with a key and the key itself is encrypted using an AES 256 rotating master key
    - AWS Key Management Service, Managed Keys - SSE-KMS
      - same as above
      - also has envelope key for protecting the key
      - provides audit trail when key was used and who used it, additional charges are incurred
    - Server Side Encryption with customer provided keys - SSE-C
      - you manage keys, aws manages encryption/decryption
  - client side encryption - you encrypt the data and aws only stored it

**S3 performance**
- **prefixes are used to make reads perform faster**
- prefixes are like different paths -
  - /mybucket/folder1/subfolder1/a.txt - prefix - folder1/subfolder1
  - /mybucket/folder2/subfolder2/b.txt - prefix - folder2/subfolder2
  - /mybucket/folder3/c.txt - prefix - folder3
- **aws processes 3500 PUT/POST/DELETE and 5500 GET/HEAD requests per second per prefix**
- if objects are scattered in more prefixes, s3 performance will be higher
- in above case, you can achieve 5500*3 = 16500 requests per second
- **SSE KMS has limits** - each upload/download will make a call to KMS to generate key
- KMS limits is region specific - 5500 or 10000 or 30000 requests per second
- currently KMS limit cannot be increased
- **for files size > 100MB, use multi part upload**
- it will parallelize uploads for bigger files
- **S3 Byte range fetches**
  - used for efficient download
  - can specify byte ranges to download in parallel
  - can be used to download partially - eg, first 64k bytes can only contain header

**S3 Select**
- enables to fetch only a subset of data from object using SQL like expression
- only downloads data that match the expression
- improves upto 400% in performance
- **also saves money on data transfer as only subset is downloaded**
- **Glacier Select** - same as S3 select, but runs on Glacier

**AWS Organisations and Consolidated Billing**
- AWS org
  - account management service that consolidates different accounts into an org and manage centrally
  - Root account -> OU -> accounts
  - e.g -> NAB -> Finance, Engg -> Engg -> Prod, Dev, test
- Consolidate billing
  - combines multiple bills under one account
  - helps in getting volume discount
- Enable/Disable aws services using Service Control Policies (SCP) either on OUs or individual accounts

**S3 Cross Account access**
- Using bucket policies and IAM
  - applies to entire bucket
  - only through programmatic access
- Using Bucket ACLs and IAM
  - applies to individual objects
  - only through programmatic access
- Using cross account IAM roles
  - programmatic and console access

**S3 Cross Region Replication**
- when a bucket is replicated, the objects are not replicated automatically. they are replicated when the objects are modified  
- **versioning must be enabled on both source and destination buckets**
- **destination bucket can be in same account or different account**
- though source bucket objects may be public, when they are replicated, they are **NOT** public by default
- if permissions on source bucket object are changed, they are **NOT** replicated to the destination bucket
- regions in source and destination should be different
- storage classes can be different between source and destination buckets
- **note that if One region IA storage class is chosen for destination bucket, replication will not work**
- existing objects in the source bucket will not be automatically copied over to new replicated bucket - has to be done manually
- all objects added/modified after replication is enabled will be replicated automatically
- **deletes (both delete marker and versions) are not replicated to the other region - this is per design to prevent accidental cross region deletion**
- cannot be replicated to multiple buckets or use daisy chaining ???
- to replicate existing objects, we have to use command line -      
```
sundeep@sundeep-lx:~$ aws configure
AWS Access Key ID [None]: AKIAJPROTIVX54U7NPBA
AWS Secret Access Key [None]: ********
Default region name [None]: us-east-1
Default output format [None]:
sundeep@sundeep-lx:~$ aws s3 ls
2019-03-20 06:21:23 smalali
2019-03-21 05:49:08 smalali-version-bucket-sydney
2019-03-20 06:45:29 smalaliversionbucket
sundeep@sundeep-lx:~$ aws s3 cp --recursive s3://smalaliversionbucket s3://smalali-version-bucket-sydney
copy: s3://smalaliversionbucket/DiceRoller.java to s3://smalali-version-bucket-sydney/DiceRoller.java
copy: s3://smalaliversionbucket/hello.txt to s3://smalali-version-bucket-sydney/hello.txt
copy: s3://smalaliversionbucket/Spring_Cloud.jpg to s3://smalali-version-bucket-sydney/Spring_Cloud.jpg
sundeep@sundeep-lx:~$
```

**S3 Transfer acceleration**
- enables fast, secure file transfer over long distances between user and S3 bucket
- makes use of Amazon's CloudFront's(CDN) globally distributed edge locations
- e.g., when user uploads a file, it is stored in the edge location (small data centre) closest to the user
- from there on, it is routed to S3 over an optimized network path
- **URL for edge location : smalali2-accelerate.s3-accelerate.amazonaws.com**

**S3 Datasync**
- used to transfer large data from on-prem to S3
- install datasync agent on prem
- sync can be scheduled hourly, daily or weekly
- used with NFS or SMB compatible file systems
- can be used to sync EFS to EFS - install agent in the EC2 instance

**Storage Gateway**
- connects on-premise software appliance to cloud storage (s3) to securely store data in aws cloud
- it is a virtual client installed on vm in data center - it replicates data to s3 or glacier
- types of storage gateway
  - file gateway (NFS / SMB)
    - stores flat files like text file, pics, videos etc
    - directly stored in s3 buckets
    - so lifecycle mgmt, bucket policies, cross-region replication apply to the objects in the bucket like normaly
    - ownership, timestamp etc are stored as object metadata
  - volume gateway (iSCSI)
    - block storage, virtual hard disc
    - used for running OS, DB etc
    - not stored in S3
    - can be asynchronously backed-up as point-in-time snapshots and stored as EBS snapshots
    - stored incrementally to minimise storage charges
    - stored volumes
      - store the entire copy of data set on-premise
      - can be mounted as iSCSI devices on your on-prem application servers
      - async backup to s3 as EBS snapshot
      - 1GB to 16TB size
    - cached volumes
      - stores the frequently accessed data in local storage gateway, rest is backed up in s3
      - minimizes need to scale local storage infrastructure
      - can be mounted as iSCSI devices on your on-prem application servers
      - 1GB to 32TB
  - tape gateway(VTL - Virtual Tape Library)
    - back up and archiving solution
    - create virtual tape and send it to s3
      - then use lifecycle policies to send them to glacier
      - uses backup apps like NetBackup, Backup Exec, Veeam etc

**Athena**
- interactive query service to use SQL like queries to fetch data from S3
- pay per query , or per TB
- no need for ETL to query S3 data
- works directly on S3 data
- can be used to -
  - query data in log files like ELB logs, S3 access logs
  - generate business reports from S3 data
  - analyse aws usage and cost
  - run queries on click-stream data

**Macie**
- service that uses ML and NLP to discover, classify and analyse PII data in S3
- PII - Personally Identifiable Information
- uses AI to recognise any senitive data in S3
- can generate reports and alerts in such cases
- can also analyse CloudTrail logs for recognising sensitive data
- helps in maintaining PCCI-DSS commpliance

**Snowball**
- petabyte scale data transport solution
- uses secure hardware appliances to transfer data
- typically used to transfer data between data centres
- data is copied to the appliance using aws local intranet thereby bypassing internet
- this reduces time taken to copy, no costly high speed internet connection etc
- appliance supports upto 50TB - 80TB
- uses tamper resistant enclosures, 256 bit encryption, industry standard Trusted Platform Module (TPM) etc
- after data is transferred, aws performs software erasure of the appliance
- **can import to S3 and export from S3**

**Snowball Edge**
- upto 100Tb storage
- acts as both storage and compute
- support local workloads in remote and offline locations
- can run lambdas on it
- can integrate with existing standard storage interfaces
- can cluster together to form local storage and process data on-prem

**Snowmobile**
- exabyte scale data transfer service
- 100PB per snowmobile
- uses a shipping container pulled by a trailer truck to move data
- used to move massive data like entire data centre, videos etc

**Import/Export Disk**
- predates Snowball
- clients used to send hard disk of data to aws and aws used to restore it on the cloud using internal high speed network (bypass internet)
- issue was clients sent different types of disks making it difficult to manage the import/export - hence Snowball was introduced


# CloudFront
**CDN**
- content delivery network
- system of distributed servers that delivers web content to user based on location, origin of web page and content delivery server

**Edge location**
- location where content will be cached
- different from AWS region/AZ
- **they are not only for read, but can be used to write objects - will be synced to origin server**
- objects are cached for duration of TTL
- **sometimes you can force flush the cache before ttl expiration - this is charged**

**origin**
- the origin of all files that a cdn will distribute - can be s3 bucket, ec2 instance, ELB, or route53

**distribution**
- the cdn which consists of a collection of edge locations
- **web distribution** - used to distribute websites
- **rtmp - for media distribution (adobe's rtmp protocol for flash files)**
- there can be multiple origins in the same distribution

**Cloudfront**
- used to deliver entire website including static/dynamic content
- uses global network of edge locations
- **is not region based, but global**
- requests are auto routed to the nearest edge location
- optimized to work with s3, ec2, elb, route53
- it can also work with non-aws origin servers containing the original files
- you can create invalidations on the distribution - this will invalidate the cache for selected objects
- cloudfront is not included in the free tier

**Cloudfront Signed URLs and Cookies**
- Signed url is for individual files
- Cookies is for multiple files
- can attach a poolicy to the url or cookie
- Cloudfront Signed URL - policy can include
  - url expiration
  - IP ranges of clients
  - trusted signers - which aws accounts can create signed urls
  - **can be used if origin is EC2 and users access using OAI**
- ![alt text](CF_Signed_URL.png "Cloudfront Signed URL")
- S3 Signed URL -
  - issues a request as the IAM user who creates the signed url
  - limited lifetime
  - **can be used when user has direct access to S3 bucket**
